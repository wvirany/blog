---
title: "Learning Representations of Proteins"  
date: "2024-12-09"  #2024-12-20
summary: ""  
description: ""  
draft: true  
toc: true  
readTime: true  
autonumber: false  
math: true  
tags: []
showTags: false  
hideBackToTop: false
---

In this blog, I build a VQ-VAE for for protein structure tokenization which learns meaningful representations of proteins.


## Variational Autoencoders

First, I'll start with an overview of Variational Autoencoders (VAE), as the VQ-VAE is a natural extension. 

A VAE consists of two main components: an encoder and a decoder network. The encoder network projects samples into a low-dimensional latent space[^fn1], which usually takes the form of a standard Gaussian, whereas the decoder network reconstructs data samples from these low-dimensional representations. Thus, the training objective aims to maximize the likelihood of generated samples coming from the true data distribution with the decoder while accurately modeling the latent space with the encoder. 

Once trained, new data points can be generated by sampling from the latent distribution, and passing samples through the decoder. Alternatively, the encoder network can be used to produce meaningful low-dimensional representations of data, which is useful for data compression, representation learning, and other downstream tasks.

### Variational Inference

Suppose our data $\mathbf{x}$ is generated by some random process, which depends on a continuous random variable $\mathbf{z}$. First, $\mathbf{z}$ is sampled from a prior distribution $p(\mathbf{z})$. Then, the data samples are generated from a conditional distribution $p(\mathbf{x}|\mathbf{z})$. We assume both of these distributions to be Gaussian. Unfortunately, we don't know these underlying distributions, nor can we observe the latent variables $\mathbf{z}$. However, we *can* approximate them.

Given observations $\mathbf{x}$, we can compute the posterior of the latent distribution $p(\mathbf{z|x})$. This can be used to gain information about the true latent variable distribution. Using Bayes' formula, we can write

$$
p(\mathbf{z}|\mathbf{x}) = \frac{p(x|z)p(z)}{p(x)}.
$$

However, this depends on the the marginal distribution of the data $p(\mathbf{x})$, the computation of which involves integrating all possible values of $\mathbf{z}$:

$$
p(\mathbf{x}) = \int p(\mathbf{x}|\mathbf{z}) p(\mathbf{z}) d\mathbf{z}.
$$

Instead of trying to compute the posterior distribution, we can instead approximate it. This is done via our encoder network, which we denote $q_{\theta}({\mathbf{z}})$. Thus, we try to minimize the [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the approximate latent density and the posterior distribution:

$$
q_{\theta}^{\*}(\mathbf{z}) \in \argmin_{\theta} D_{KL} (q_{\theta}(\mathbf{z}) || p(\mathbf{z}|\mathbf{x})).
$$

Of course, we don't know the posterior. However, we can rewrite this as

$$
\begin{align*}
D_{KL} (q_{\theta}(\mathbf{z}) || p(\mathbf{z}|\mathbf{x})) &= \mathbb{E} \left[ \log \left( \frac{q_{\theta}(\mathbf{z})}{p(\mathbf{z}|\mathbf{x})} \right) \right] \\\\
&= \mathbb{E} \left[ \log q_\theta(\mathbf{z}) \right] - \mathbb{E} \left[ \log p(\mathbf{z} | \mathbf{x}) \right] \\\\
&= \mathbb{E} \left[ \log q_{\theta}(\mathbf{z}) \right] - \mathbb{E} \left[ \log p(\mathbf{z}, \mathbf{x}) \right] + \mathbb{E} \left[ \log p(\mathbf{x}) \right],
\end{align*}
$$

where the expectation is with respect to $\mathbf{z} \sim q_{\theta}(\mathbf{z})$. Thus, we can drop the expectation around the last term, since it is independent of $\mathbf{z}$. By rearranging this, we see that

$$
\log p(\mathbf{x}) - D_{KL} (q_{\theta}(\mathbf{z}) || p(\mathbf{z}|\mathbf{x})) = \mathbb{E} \left[ \log p(\mathbf{z}, \mathbf{x}) \right] - \mathbb{E} \left[ \log q_{\theta}(\mathbf{z}) \right].
$$

We see that by maximizing the RHS, we simultaneously maximize the likelihood of the data while minimizing the KL divergence between the the true and approximate posterior distributions. We define this to be the evidence lower bound (ELBO), which we use as our loss function in training[^fn2].

To compute the ELBO


### A Simple Example

Here I'll implement a simple example of a VAE for image generation. We'll use this later as the foundation for our VQ-VAE. First, I'll import some useful packages and configure my device:

<!-- IMPORT PACKAGES -->
```py
import torch
import torch.nn as nn
from torch.utils.data.dataloader import DataLoader
from torchvision import datasets, transforms
import cv2
from tqdm import tqdm

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

I'll also download the MNIST dataset and create a `DataLoader` object:

```py
batch_size = 64

# Transform images to tensors
transform = transforms.Compose([
    transforms.ToTensor()
])

# Download and load MNIST
train_dataset = datasets.MNIST(
    root='./data',  # Where to store the dataset
    train=True,     # Use training set
    download=True,  # Download if not present
    transform=transform
)

# Create DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)
```


Now, I'll start by implementing the encoder network. This takes an image as input, then predicts the mean and log-variance of the latent distribution:


```py
class Encoder(nn.Module):
    def __init__(self, input_dim=28*28, latent_dim=2):
        super().__init__()

        self.common_MLP = nn.Sequential(
            nn.Linear(input_dim, 196),
            nn.Tanh(),
            nn.Linear(196, 128),
            nn.Tanh(),
            nn.Linear(128, 64)
        )

        self.mean_MLP = nn.Sequential(
            nn.Linear(64, 16),
            nn.Tanh(),
            nn.Linear(16, latent_dim)
        )

        self.var_MLP = nn.Sequential(
            nn.Linear(64, 16),
            nn.Tanh(),
            nn.Linear(16, latent_dim)
        )
    
    def forward(self, x):

        B = x.shape[0]
        x = x.view(B, -1) # Flattens to (batch_size, 784)

        x = self.common_MLP(x)

        mean = self.mean_MLP(x)
        log_var = self.var_MLP(x)

        return mean, log_var
```

Next, I implement the decoder network. This takes a latent vector as input and produces the reconstructed image:

```py
class Decoder(nn.Module):
    def __init__(self, latent_dim=2, output_dim=28*28):
        super().__init__()

        self.latent_dim = latent_dim
        self.output_dim = output_dim

        self.layers = nn.Sequential(
            nn.Linear(latent_dim, 16),
            nn.Tanh(),
            nn.Linear(16, 64),
            nn.Tanh(),
            nn.Linear(64, 128),
            nn.Tanh(),
            nn.Linear(128, 256),
            nn.Tanh(),
            nn.Linear(256, output_dim)
        )
    
    def forward(self, x):

        for layer in self.layers:
            x = layer(x)

        # For 2D square image, get dimension
        dim = int(self.output_dim**.5)
        
        return x.view(x.shape[0], 1, dim, dim)
```

We can put this together into a full VAE:

```py
class VAE(nn.Module):
    def __init__(self, input_dim=28*28, output_dim=28*28, latent_dim=2):
        super().__init__()

        self.encoder = Encoder(input_dim=input_dim, latent_dim=latent_dim)
        self.decoder = Decoder(latent_dim=latent_dim, output_dim=output_dim)

    def sample(self, mean, log_var):

        # Convert logvar to std
        std = torch.exp(0.5 * log_var)
        # Sample standard Normal
        epsilon = torch.randn_like(std)
        # z is Normal w/ mean and var as predicted
        # Note: Var(std * eps) = std^2 * Var(eps) = std^2
        z = mean + std * epsilon

        return z

    def forward(self, x):

        mean, log_var = self.encoder(x)

        z = self.sample(mean, log_var)

        out = self.decoder(z)

        return mean, log_var, out
```

This encodes an image into latent space by sampling from a Normal distribution with mean and variance as predicted by the encoder network. Then, it produces a reconstructed image by passing the latent vector through the decoder network.

Finally, we define the training function:
```py
def train_vae(model, data_loader, num_epochs=10, learning_rate=1e-3):

    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    criterion = torch.nn.MSELoss()

    for t in range(num_epochs):
        
        for im, label in tqdm(data_loader):
            im = im.float().to(device)

            optimizer.zero_grad()
            
            # Forward pass
            mean, log_var, out = model(im)

            # Compute Losses
            kl_loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())
            reconstruction_loss = criterion(out, im)
            loss = reconstruction_loss + 0.00001 * kl_loss

            # Backward pass
            loss.backward()
            optimizer.step()

        # Total loss averaged over batch 
        print(f"Epoch: {t+1} | Loss: {torch.mean(loss):.4f}")

    print('Done Training ...')
```

```py
model = VAE()

train_vae(model, train_loader, num_epochs=10)
```

## VQ-VAEs

### Implementation

## Learning the Representations of Proteins

## References and Further Reading

1.  A. v. d. Oord, O. Vinyals, and K. Kavukcuoglu. [Neural Discrete Representation Learning.](http://arxiv.org/abs/1711.00937). NeurIPS 2017.

2. ESM3 (or ESMFold)


[^fn1]: The term "latent" means hidden; we generally cannot observe these variables.

[^fn2]: The marginal distribution $p(\mathbf{x})$ is also known as the "evidence". Thus, since the KL divergence is always positive, we see that $\log p(\mathbf{x}) \geq \text{ELBO}$, hence the name.