---
title: "Protein Representation Learning with a VQ-VAE"  
date: ""  
summary: ""  
description: ""  
draft: true  
toc: false  
readTime: true  
autonumber: false  
math: true  
tags: []
showTags: false  
hideBackToTop: false
---

In this blog, I build a VQ-VAE for for protein structure tokenization which learns meaningful representations of proteins.


## Variational Autoencoders

First, I'll start with an overview of Variational Autoencoders (VAE), as the VQ-VAE is a natural extension. 

A VAE consists of two main components: an encoder and a decoder network. The encoder network projects samples into a low-dimensional latent space[^fn1], which usually takes the form of a standard Gaussian, whereas the decoder network reconstructs data samples from these low-dimensional representations. Thus, the training objective aims to maximize the likelihood of generated samples coming from the true data distribution with the decoder while accurately modeling the latent space with the encoder. 

Once trained, new data points can be generated by sampling from the latent distribution, and passing samples through the decoder. Alternatively, the encoder network can be used to produce meaningful low-dimensional representations of data, which is useful for data compression, representation learning, and other downstream tasks.

#### Variational Inference and the Evidence Lower Bound

Suppose our data $\mathbf{x}$ is generated by some random process, which depends on a continuous random variable $\mathbf{z}$. First, $\mathbf{z}$ is sampled from a prior distribution $p(\mathbf{z})$. Then, the data samples are generated from a conditional distribution $p(\mathbf{x}|\mathbf{z})$. We assume both of these distributions to be Gaussian. Unfortunately, we don't know these underlying distributions, nor can we observe the latent variables $\mathbf{z}$. However, we *can* approximate them.

Given observations $\mathbf{x}$, we can compute the posterior of the latent distribution $p(\mathbf{z|x})$. This can be used to gain information about the true latent variable distribution. Using Bayes' formula, we can write

$$
p(\mathbf{z}|\mathbf{x}) = \frac{p(x|z)p(z)}{p(x)}.
$$

However, this depends on the the marginal distribution of the data $p(\mathbf{x})$, the computation of which involves integrating all possible values of $\mathbf{z}$:

$$
p(\mathbf{x}) = \int p(\mathbf{x}|\mathbf{z}) p(\mathbf{z}) d\mathbf{z}.
$$

Instead of trying to compute the posterior distribution, we can instead approximate it. This is done via our encoder network, which we denote $q_{\theta}({\mathbf{z}})$. Thus, we try to minimize the [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the approximate latent density and the posterior distribution:

$$
q_{\theta}^{\*}(\mathbf{z}) \in \argmin_{\theta} D_{KL} (q_{\theta}(\mathbf{z}) || p(\mathbf{z}|\mathbf{x})).
$$

Of course, we don't know the posterior. However, we can rewrite this as

$$
\begin{align*}
D_{KL} (q_{\theta}(\mathbf{z}) || p(\mathbf{z}|\mathbf{x})) &= \mathbb{E} \left[ \log \left( \frac{q_{\theta}(\mathbf{z})}{p(\mathbf{z}|\mathbf{x})} \right) \right] \\\\
&= \mathbb{E} \left[ \log q_\theta(\mathbf{z}) \right] - \mathbb{E} \left[ \log p(\mathbf{z} | \mathbf{x}) \right] \\\\
&= \mathbb{E} \left[ \log q_{\theta}(\mathbf{z}) \right] - \mathbb{E} \left[ \log p(\mathbf{z}, \mathbf{x}) \right] + \mathbb{E} \left[ \log p(\mathbf{x}) \right],
\end{align*}
$$

where the expectation is with respect to $\mathbf{z} \sim q_{\theta}(\mathbf{z})$. Thus, we can drop the expectation around the last term, since it is independent of $\mathbf{z}$. By rearranging this, we see that

$$
\log p(\mathbf{x}) - D_{KL} (q_{\theta}(\mathbf{z}) || p(\mathbf{z}|\mathbf{x})) = \mathbb{E} \left[ \log p(\mathbf{z}, \mathbf{x}) \right] - \mathbb{E} \left[ \log q_{\theta}(\mathbf{z}) \right].
$$

We see that by maximizing the RHS, we simultaneously maximize the likelihood of the data while minimizing the KL divergence between the the true and approximate posterior distributions. We define this to be the evidence lower bound (ELBO), which we use as our loss function in training[^fn2].

To compute the ELBO

### References and Further Reading

1.  A. v. d. Oord, O. Vinyals, and K. Kavukcuoglu. [Neural Discrete Representation Learning.](http://arxiv.org/abs/1711.00937). NeurIPS 2017.

2. ESM3 (or ESMFold)


[^fn1]: The term "latent" means hidden; we generally cannot observe these variables.

[^fn2]: The marginal distribution $p(\mathbf{x})$ is also known as the "evidence". Thus, since the KL divergence is always positive, we see that $\log p(\mathbf{x}) \geq \text{ELBO}$, hence the name.