---
title: "Generating Synthesizable Molecules (1/2)"  
date: "" # 2024-01-10
summary: "Part 1: Projecting Molecules into Synthesizable Chemical Spaces"  
description: ""  
draft: true  
toc: false  
readTime: true  
autonumber: false  
math: true  
tags: ["code, drug discovery"]
showTags: false  
hideBackToTop: false
---

A significant challenge in the use of generative modeling for molecular design is the problem of synthesizability. That is, a fancy deep learning model might make a molecule that checks all of our boxes in terms of desirable properties, but can it actually be synthesized in a lab? If the answer is no, then that *in silico* molecule is not very useful.

There are many attempts to overcome this problem, but one elegant approach is the work of [Luo et al. [2024]](https://arxiv.org/pdf/2406.04628), in which they "project" molecules into synthesizable chemical space.
This effectively boils down to a machine translationt task, in which a generated molecule is translated via a transformer architecture to its closest synthesizable relative.

In this blog post, I aim to review the work of [Luo et al. [2024]](https://arxiv.org/pdf/2406.04628). The first ~half is essentially a tutorial on implementing their work. The second half explores experiments?

something something lightweight / simple tutorial for demonstration and reproducibility?

## Preliminaries

First, let's import the necessary packages that we use:

```py
import torch
import torch.nn as nn
import torch.nn.functional as F

from rdkit import Chem
from rdkit.Chem import AllChem, rdChemReactions

from typing import List, Dict, Union
```



### Defining Chemical Space

We define a chemical space $\mathcal{C}$ as a set of molecules generated by a starting set of building block molecules, denoted

$$
\mathcal{B} = \\{B_1, B_2, \dots, B_N\\},
$$

and a set of reaction rules, denoted

$$
\mathcal{R} = \\{R_1, R_2, \dots, R_M\\}.
$$

A reaction rule can be formalized as a function which maps a set of reactants to a set of reaction products.
For example, a reaction rule $R$ which takes two reactants can be written as

$$
\begin{align*}
R: \mathcal{X_1} \times \mathcal{X_2} &\to \mathcal{C}\\\\
(X_1, X_2) &\mapsto Y.
\end{align*}
$$

Here, $\mathcal{X_1}$ and $\mathcal{X_2}$ are sets of molecules to which reaction $R$ can be applied, and $Y$ is the main reaction product[^fn1]. $\mathcal{C}$ is then constructed by applying each of the reactions in $\mathcal{R}$ iteratively to every possible combination of molecules in $\mathcal{B}$.

Let's implement this.
We start by defining `Molecule` and `Reaction` classes:

```py
class Molecule:
    def __init__(self, smiles: str):
        self.smiles = smiles
        self._rdmol = Chem.MolFromSmiles(smiles)
    
    @property
    def rdmol(self):
        return self._rdmol

class Reaction:
    def __init__(self, smarts: str):
        self.smarts = smarts
        self._reaction = AllChem.ReactionFromSmarts(smarts)

        # Need to initialize the reaction
        rdChemReactions.ChemicalReaction.Initialize(self._reaction)
    
    def apply(self, reactants: List[Molecule]) -> List[Molecule]:
        reactant_mols = [r.rdmol for r in reactants]
        products = self._reaction.RunReactants(reactant_mols)
        if products:
            # Return main product
            return [[Molecule(Chem.MolToSmiles(p)) for p in products[0]][0]]
        return []

    def can_apply(self, molecules: List[Molecule]) -> bool:
        if len(molecules) != self.num_reactants:
            return False
        return any(self._reaction.RunReactants([m.rdmol] for m in molecules))
    
    @property
    def num_reactants(self) -> int:
        return self._reaction.GetNumReactantTemplates()
```

Let's also define the `ChemicalSpace` class[^fn2]:

```py
class ChemicalSpace:
    def __init__(self, building_blocks: List[Molecule], reactions: List[Reaction]):
        self.building_blocks = building_blocks
        self.reactions = reactions

        # Possible reactions for each molecule are stored as a list of indices in a dictionary
        self.reaction_index: Dict[str, List[int]] = self._build_reaction_index()
    
    def _build_reaction_index(self) -> Dict[str, List[int]]:
        index = {}
        
        # For each molecule, get the indices of the reactions which can be applied to it
        for mol in self.building_blocks:
            applicable_reactions = [j for j, r in enumerate(self.reactions) if r.can_apply([mol])]
            index[mol.smiles] = applicable_reactions
        return index
    
    def get_applicable_reactions(self, mol: Molecule) -> List[Reaction]:
        # Get list of reaction indices for given molecule
        indices = self.reaction_index.get(mol.smiles, [])

        # Return the reactions specified by the given indices
        return [self.reactions[i] for i in indices] 
```

### Postfix Notation of Synthesis

For a guide to reactions in RDKit, I recommend these:
1. https://github.com/rdkit/rdkit-tutorials/blob/master/notebooks/003_SMARTS_ReactionsExamples.ipynb
2. https://www.rdkit.org/docs/GettingStartedInPython.html#chemical-reactions


### Representing Molecules as Graphs

The transformer architecture which we will develop later encodes molecular graph representations.
Here, I show a simple example of how we represent molecules as graphs.

First, let's create a simple molecule; ethanol:

```py
# Create a molecule
ethanol = Molecule("CCO")
```

We ultimately want to "featurize" the molecule.
That is, we want to represent the molecule in a format suitable to be input to our transformer.
For this simple example, we represent atoms as their atomic number. Similarly, we featurize the bonds by creating an adjacency matrix which defines each atom in the molecule is related.
For example, the element in row $i$ and column $j$ indicates the type of bond between atoms $i$ and $j$ in the molecule.

We can access the RDKit molecule object via the `rdmol` attribute we defined earlier.
This object comes equipped with a suite of functionality from the RDKit package.

```py
mol = ethanol.rdmol
num_atoms = mol.GetNumAtoms()

# Atom features
atoms = torch.zeros(num_atoms, dtype=torch.float32)
for i in range(num_atoms):
    atom = rdmol.GetAtomWithIdx(i)
    atoms[i] = atom.GetAtomicNum()

# Bond features
# 0: no bond, 1: single bond, 1.5: aromatic bond, 2: double bond
bonds = torch.zeros((num_atoms))
for bond in mol.GetBonds():
    i = bond.GetBeginAtomIdx()
    j = bond.GetEndAtomIdx()

    # Convert bond types to integers
    bond_type = mol.GetBondType()
    if bond_type == Chem.rdchem.BondType.SINGLE:
        return 1
    elif bond_type == Chem.rdchem.BondType.DOUBLE:
        bond_type = 2
    elif bond.GetBondType() == Chem.rdchem.BondType.TRIPLE:
        bond_type = 3
    elif bond.GetBondType() == Chem.rdchem.BondType.AROMATIC:
        bond_type = 4

    bonds[i, j] = bond_type
    bonds[j, i] = bond_type
```

Now, the information describing the atoms and bonds in the molecule are stored as tensors:

```
Atoms tensor:
tensor([6, 6, 8])  # C, C, O

Bonds matrix:
tensor([[0, 1, 0],   # single bond between C-O
        [1, 0, 1],   # single bonds between C-O and O-O
        [0, 1, 0]])  # single bond between O-O
```


## Building the Model

The model consists of two main components: 

### A Simple Graph Encoder

First, we build a simple graph encoder network to see how the model processes the molecular graph representations.

```py
class EmbeddingLayer(nn.Module):
    def __init__(self, num_atom_classes=100, num_bond_classes=5, dim=8):
        super().__init__()
        self.atom_emb = nn.Embedding(num_atom_classes, dim, padding_idx=0)
        self.bond_emb = nn.Embedding(num_bond_classes, dim, padding_idx=0)
    
    def forward(self, atoms, bonds):
        node_features = self.atom_emb(atoms)
        edge_features = self.bond_emb(bonds)
        return node_features, bond_features
```

So far, this shows how we pass molecular graphs through the embedding layer. Let's explore how this works:

```py
# Create embedding layer
embed = EmbeddingLayer()

atoms = torch.tensor([6, 6, 8])  # C, C, O
bonds = torch.tensor([[0, 1, 0],
                     [1, 0, 1],
                     [0, 1, 0]])

# Get embeddings
node_features, edge_features = embed(atoms, bonds)
```

Now, the input has been transformed into the initial embeddings, replacing each of the atomic and bond features with 8-dimensional vectors:

```
Atom embeddings shape: torch.Size([3, 8])
Bond embeddings shape: torch.Size([3, 3, 8])
```

We see that the shape of the atom embeddings tensor is (3, 8), indicating that there are 3 atoms in the molecule, each with an 8-dimensional embedding vector describing the type of atom.
Similarly, the shape of the bond embeddings tensor is (3, 3, 8), now indicating that each element of the 3 $\times$ 3 adjacency matrix is likewise an 8-dimensional vector.

Now, all we've done so far is passed the molecules through the embedding layer.
Next, we build the transformer.
This will involve passing the embedded input through a sequence of several encoding layers.
Let's see how this encoder layer works&nbsp;[^fn3]&nbsp;:

```py
class EncoderLayer(nn.Module):
    def __init__(self, d_model=64, nhead=8):
        super().__init__()

        # Multi-head self attention
        self.self_attn = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=nhead,
            batch_first=True
        )

        # Feed Forward Network
        self.linear1 = nn.Linear(d_model, d_model*4)
        self.linear2 = nn.Linear(d_model*4, d_model)

        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
    
    def forward(self, x, padding_mask=None):
        # Self Attention Block
        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=padding_mask)
        x = x + attn_out
        x = self.norm1(x)

        # Feed Forward Block
        ff_out = self.linear2(F.relu(self.linear(x)))
        x = x + ff_out
        x = self.norm2(x)

        return x
```

This is just a conventional encoder layer as defined in [Vaswani et al., [2017]](https://arxiv.org/pdf/1706.03762), with a multi-head self-attention mechanism, followed by an MLP, with layer normalization and residual connections in between. This just takes our embedding vectors as input, and returns vectors of the same shape:

```py
d_model = 8
encoder_layer = EncoderLayer(d_model=d_model, nhead=2)

output = layer(atoms)
```

Here, we used the `atoms` tensor from the previous example. Now, the output is the same shape as the original embedded representation:

```
Output shape: torch.Size([3, 8])
```

All that's left is to stack these encoder layers in a sequence.
We put this together in the following `GraphEncoder` class:

```py
class GraphEncoder(nn.Module):
    def __init__(self, num_atom_classes=100, num_bond_classes=5, dim=8, n_layers=2, n_heads=2):
        super().__init__()
        self.atom_emb = nn.Embedding(num_atom_classes, dim, padding_idx=0)
        self.bond_emb = nn.Embedding(num_bond_classes, dim, padding_idx=0)

        # Add transformer layers
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=dim,
            nhead=n_heads,
            batch_first=True
        )
        # Stack encoder layers in sequence
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=n_layers
        )

    def forward(self, atoms, bonds, padding_mask=None):
        # Get Embeddings
        node_features = self.atom_emb(atoms)    # (num_atoms, dim)
        edge_features = self.bond_emb(bonds)    # (num_atoms, num_atoms, dim)

        # Combine atom and bond information
        # For each atom, aggregate bond information from its neighbors
        edge_sum = edge_features.sum(dim=1)     # (num_atoms, dim)
        node_features = node_features + edge_sum

        # Pass through transformer
        out = self.transformer(
            node_features,
            src_key_padding_mask=padding_mask
        )

        return out
```

Again, let's see how to use this:

```py
encoder = GraphEncoder()

atoms = torch.tensor([6, 6, 8])  # C, C, O
bonds = torch.tensor([[0, 1, 0],
                      [1, 0, 1],
                      [0, 1, 0]])

encoded = encoder(atoms, bonds)
```

Our output takes the shape:

```
Encoded molecule shape: torch.Size([3, 8])
```


### Postfix Notation Decoder



### Training

### References and Further Reading

1. Shitong Luo, Wenhao Gao, Zuofan Wu, Jian Peng, Connor W. Coley, and Jianzhu Ma. Projecting molecules into
synthesizable chemical spaces. In Forty-first International Conference on Machine Learning, 2024.



[^fn1]: It is sometimes the case that a reaction has multiple main reaction products. For the sake of clarity in our discussion, I am assuming there is only one main reaction product. However, the original authors' implementation handles the more general case.

[^fn2]: We use reaction list, they use reaction matrix.

[^fn3]: After the initial embeddings, the original paper takes "edges into account by adding a bond type-dependent learnable bias term to the query-key product matrix".
We'll simplify this detail; instead by accounting for bond information by simply adding the `node_features` and `edge_features` embeddings together.